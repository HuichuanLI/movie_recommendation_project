{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:46:03.364630Z",
     "start_time": "2022-02-09T15:46:02.877023Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:46:58.235213Z",
     "start_time": "2022-02-09T15:46:58.229130Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'all_rating_min_max',\n",
    "        'members_min_max',\n",
    "        'aired_from_min_max',\n",
    "        'aired_to_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()\n",
    "\n",
    "def get_user_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'user_rating_ave_min_max',\n",
    "        'user_rating_std_min_max',\n",
    "        'user_aired_from_ave_min_max',\n",
    "        'user_aired_to_ave_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()\n",
    "\n",
    "def get_multihot_feature(df, feat_name):\n",
    "    feat_df = df[[feat_name]]\n",
    "    feat_vecs = feat_df.to_numpy()\n",
    "    feat_vec = np.apply_along_axis(lambda v: v[0], 1, feat_vecs)\n",
    "    return feat_vec\n",
    "\n",
    "def get_label(df):\n",
    "    label_df = df[['label']]\n",
    "    return label_df.to_numpy()\n",
    "\n",
    "def get_all_features(df):\n",
    "    return ( \n",
    "        get_multihot_feature(df, 'genres_multihot'),\n",
    "        get_multihot_feature(df, 'user_liked_genres_multihot'),\n",
    "        get_item_numeric_features(df),\n",
    "        get_user_numeric_features(df) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:46:06.745848Z",
     "start_time": "2022-02-09T15:46:03.374452Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorboard.plugins.hparams import api as hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:46:06.750135Z",
     "start_time": "2022-02-09T15:46:06.747652Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:46:06.755840Z",
     "start_time": "2022-02-09T15:46:06.752120Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('../../data/anime/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:46:06.761303Z",
     "start_time": "2022-02-09T15:46:06.757560Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = data_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:59:53.386526Z",
     "start_time": "2022-02-09T15:59:53.381920Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "HP_LAYERS = hp.HParam(\"layers\", hp.IntInterval(2, 3))\n",
    "HP_LAYER_SIZE = hp.HParam(\"layer_size\", hp.Discrete([64, 128, 256]))\n",
    "HP_LEARN_RATE = hp.HParam(\"learn_rate\", hp.Discrete([0.001, 0.003, 0.01]))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_LAYERS,\n",
    "    HP_LAYER_SIZE,\n",
    "    HP_LEARN_RATE\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:59:54.047028Z",
     "start_time": "2022-02-09T15:59:54.039849Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(x1_shape, x2_shape, x3_shape, x4_shape, hparams):\n",
    "    x1_input = keras.layers.Input(shape=(x1_shape,))\n",
    "    x2_input = keras.layers.Input(shape=(x2_shape,))\n",
    "    x3_input = keras.layers.Input(shape=(x3_shape,))\n",
    "    x4_input = keras.layers.Input(shape=(x4_shape,))\n",
    "    \n",
    "    # compact embedding for x1 and x2\n",
    "    compact_x1 = keras.layers.Dense(10)(x1_input)\n",
    "    compact_x2 = keras.layers.Dense(10)(x2_input)\n",
    "    \n",
    "    # concat all\n",
    "    merge = keras.layers.concatenate([compact_x1, compact_x2, x3_input, x4_input])\n",
    "    \n",
    "    # hidden layers\n",
    "    h_input = merge\n",
    "    for _ in range(hparams[HP_LAYERS]):\n",
    "        h = keras.layers.Dense(hparams[HP_LAYER_SIZE], activation='relu')(h_input)\n",
    "        h_input = h\n",
    "    \n",
    "    # output\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(h_input)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[x1_input, x2_input, x3_input, x4_input], outputs=output)\n",
    "    \n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE])\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:59:54.845519Z",
     "start_time": "2022-02-09T15:59:54.842944Z"
    }
   },
   "outputs": [],
   "source": [
    "test_x1s = []\n",
    "test_x2s = []\n",
    "test_x3s = []\n",
    "test_x4s = []\n",
    "test_ys = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:59:55.388484Z",
     "start_time": "2022-02-09T15:59:55.383324Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('../../data/anime/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames\n",
    "\n",
    "filenames = data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:59:55.798072Z",
     "start_time": "2022-02-09T15:59:55.791095Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model_id, hparams):\n",
    "    # build model\n",
    "    model = build_model(43, 43, 4, 4, hparams)\n",
    "    print(f\"model id: {model_id}:\")\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "    # config hparam logs\n",
    "    log_filename = f\"{model_id}\"\n",
    "    for h in hparams:\n",
    "        log_filename += f\"_{h.name}-{hparams[h]}\"\n",
    "    \n",
    "    log_dir = os.path.join(\"hparams\", log_filename)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir,\n",
    "        update_freq = 10,\n",
    "        profile_batch = 0\n",
    "    )\n",
    "    hparams_callback = hp.KerasCallback(log_dir, hparams)\n",
    "    \n",
    "    # train model\n",
    "    for filename in filenames[:1]:\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "        # shuffle and split train and test\n",
    "        train_df = df\n",
    "#         train_df = df.sample(frac=0.8, random_state=666)\n",
    "#         test_df = df.drop(train_df.index)\n",
    "\n",
    "        # get features\n",
    "        train_x1, train_x2, train_x3, train_x4 = get_all_features(train_df)\n",
    "#         val_x1, val_x2, val_x3, val_x4 = get_all_features(test_df)\n",
    "\n",
    "        # get label\n",
    "        train_y = get_label(train_df)\n",
    "#         val_y = get_label(test_df)\n",
    "\n",
    "        print('training on new dataset')\n",
    "\n",
    "        model.fit(\n",
    "            [train_x1, train_x2, train_x3, train_x4], \n",
    "            train_y, \n",
    "            validation_split=0.2,\n",
    "            batch_size=16, \n",
    "            epochs=4,\n",
    "            callbacks=[tensorboard_callback, hparams_callback]\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:59:56.198118Z",
     "start_time": "2022-02-09T15:59:56.193480Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_params():\n",
    "    with tf.summary.create_file_writer('hparams').as_default():\n",
    "            hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "            \n",
    "    model_id = 0\n",
    "    for layers in range(HP_LAYERS.domain.min_value, HP_LAYERS.domain.max_value + 1):\n",
    "        for size in HP_LAYER_SIZE.domain.values:\n",
    "            for rate in HP_LEARN_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_LAYERS: layers,\n",
    "                    HP_LAYER_SIZE: size,\n",
    "                    HP_LEARN_RATE: rate\n",
    "                }\n",
    "\n",
    "                run_model(model_id, hparams)\n",
    "                model_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-09T15:59:56.597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model id: 0:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.6122 - accuracy: 0.6583 - val_loss: 0.6052 - val_accuracy: 0.6690\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5634 - accuracy: 0.7069 - val_loss: 0.5810 - val_accuracy: 0.6896\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5498 - accuracy: 0.7182 - val_loss: 0.5786 - val_accuracy: 0.6983\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.7237 - val_loss: 0.5718 - val_accuracy: 0.6931\n",
      "model id: 1:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 3s 2ms/step - loss: 0.6099 - accuracy: 0.6616 - val_loss: 0.5962 - val_accuracy: 0.6830\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5609 - accuracy: 0.7064 - val_loss: 0.5910 - val_accuracy: 0.6793\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5493 - accuracy: 0.7184 - val_loss: 0.5789 - val_accuracy: 0.6973\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5436 - accuracy: 0.7240 - val_loss: 0.5848 - val_accuracy: 0.6888\n",
      "model id: 2:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 3s 1ms/step - loss: 0.6261 - accuracy: 0.6487 - val_loss: 0.6069 - val_accuracy: 0.6668\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5754 - accuracy: 0.6933 - val_loss: 0.5809 - val_accuracy: 0.6921\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5645 - accuracy: 0.7054 - val_loss: 0.5848 - val_accuracy: 0.6928\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5560 - accuracy: 0.7134 - val_loss: 0.5709 - val_accuracy: 0.6971\n",
      "model id: 3:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.6075 - accuracy: 0.6667 - val_loss: 0.6046 - val_accuracy: 0.6673\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5595 - accuracy: 0.7061 - val_loss: 0.5830 - val_accuracy: 0.6953\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5429 - accuracy: 0.7204 - val_loss: 0.5758 - val_accuracy: 0.6953\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5352 - accuracy: 0.7285 - val_loss: 0.5739 - val_accuracy: 0.6958\n",
      "model id: 4:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.6062 - accuracy: 0.6663 - val_loss: 0.5960 - val_accuracy: 0.6806\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5636 - accuracy: 0.7079 - val_loss: 0.5849 - val_accuracy: 0.6821\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5517 - accuracy: 0.7181 - val_loss: 0.5830 - val_accuracy: 0.6891\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5432 - accuracy: 0.7239 - val_loss: 0.5866 - val_accuracy: 0.6905\n",
      "model id: 5:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 3s 1ms/step - loss: 0.6249 - accuracy: 0.6501 - val_loss: 0.6048 - val_accuracy: 0.6648\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5782 - accuracy: 0.6973 - val_loss: 0.6047 - val_accuracy: 0.6800\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5629 - accuracy: 0.7086 - val_loss: 0.5904 - val_accuracy: 0.6841\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5553 - accuracy: 0.7162 - val_loss: 0.5728 - val_accuracy: 0.6980\n",
      "model id: 6:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 3s 2ms/step - loss: 0.6021 - accuracy: 0.6720 - val_loss: 0.5909 - val_accuracy: 0.6853\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5567 - accuracy: 0.7121 - val_loss: 0.5819 - val_accuracy: 0.6926\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5450 - accuracy: 0.7224 - val_loss: 0.5951 - val_accuracy: 0.6890\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7265 - val_loss: 0.5748 - val_accuracy: 0.6901\n",
      "model id: 7:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 3s 2ms/step - loss: 0.6032 - accuracy: 0.6711 - val_loss: 0.5857 - val_accuracy: 0.6943\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5636 - accuracy: 0.7101 - val_loss: 0.5868 - val_accuracy: 0.6908\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5525 - accuracy: 0.7145 - val_loss: 0.5753 - val_accuracy: 0.6913\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 2s 1ms/step - loss: 0.5458 - accuracy: 0.7192 - val_loss: 0.5893 - val_accuracy: 0.6893\n",
      "model id: 8:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 3s 2ms/step - loss: 0.6245 - accuracy: 0.6485 - val_loss: 0.6277 - val_accuracy: 0.6362\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 2s 2ms/step - loss: 0.5829 - accuracy: 0.6928 - val_loss: 0.6160 - val_accuracy: 0.6590\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 3s 2ms/step - loss: 0.5647 - accuracy: 0.7044 - val_loss: 0.5845 - val_accuracy: 0.6898\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 4s 2ms/step - loss: 0.5593 - accuracy: 0.7123 - val_loss: 0.5731 - val_accuracy: 0.6961\n",
      "model id: 9:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 4s 2ms/step - loss: 0.6118 - accuracy: 0.6608 - val_loss: 0.6178 - val_accuracy: 0.6608\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 3s 2ms/step - loss: 0.5626 - accuracy: 0.7055 - val_loss: 0.6240 - val_accuracy: 0.6688\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 4s 3ms/step - loss: 0.5453 - accuracy: 0.7223 - val_loss: 0.5823 - val_accuracy: 0.6896\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 4s 3ms/step - loss: 0.5383 - accuracy: 0.7248 - val_loss: 0.5836 - val_accuracy: 0.6886\n",
      "model id: 10:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 6s 4ms/step - loss: 0.6078 - accuracy: 0.6688 - val_loss: 0.6175 - val_accuracy: 0.6762\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 7s 5ms/step - loss: 0.5658 - accuracy: 0.7082 - val_loss: 0.5980 - val_accuracy: 0.6796\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 10s 7ms/step - loss: 0.5533 - accuracy: 0.7149 - val_loss: 0.5945 - val_accuracy: 0.6901\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 11s 7ms/step - loss: 0.5476 - accuracy: 0.7199 - val_loss: 0.5708 - val_accuracy: 0.6986\n",
      "model id: 11:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 9s 5ms/step - loss: 0.6266 - accuracy: 0.6476 - val_loss: 0.6211 - val_accuracy: 0.6545\n",
      "Epoch 2/4\n",
      "1503/1503 [==============================] - 5s 3ms/step - loss: 0.5861 - accuracy: 0.6872 - val_loss: 0.6053 - val_accuracy: 0.6602\n",
      "Epoch 3/4\n",
      "1503/1503 [==============================] - 5s 3ms/step - loss: 0.5718 - accuracy: 0.7017 - val_loss: 0.5942 - val_accuracy: 0.6808\n",
      "Epoch 4/4\n",
      "1503/1503 [==============================] - 4s 3ms/step - loss: 0.5607 - accuracy: 0.7105 - val_loss: 0.5805 - val_accuracy: 0.6986\n",
      "model id: 12:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1503/1503 [==============================] - 6s 3ms/step - loss: 0.6117 - accuracy: 0.6621 - val_loss: 0.5894 - val_accuracy: 0.6850\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 907/1503 [=================>............] - ETA: 1s - loss: 0.5695 - accuracy: 0.6986"
     ]
    }
   ],
   "source": [
    "test_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:50:57.194519Z",
     "start_time": "2022-02-09T15:50:46.518466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eb1db5cf26d4a14b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eb1db5cf26d4a14b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-09T16:00:28.572Z"
    }
   },
   "outputs": [],
   "source": [
    "test_x1 = np.vstack(test_x1s)\n",
    "test_x2 = np.vstack(test_x2s)\n",
    "test_x3 = np.vstack(test_x3s)\n",
    "test_x4 = np.vstack(test_x4s)\n",
    "test_y = np.vstack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-09T16:00:33.192Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate([test_x1, test_x2, test_x3, test_x4], test_y)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-09T16:00:38.042Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "n = 1\n",
    "g = 0\n",
    "\n",
    "for i in range(n):\n",
    "    expect = test_y[i][0]\n",
    "    x1 = test_x1[i]\n",
    "    x2 = test_x2[i]\n",
    "    x3 = test_x3[i]\n",
    "    x4 = test_x4[i]\n",
    "    xs = [ np.array([x1]), np.array([x2]), np.array([x3]), np.array([x4]) ]\n",
    "    print('xs')\n",
    "    print(xs)\n",
    "    \n",
    "    predict = model.predict(xs)\n",
    "    predict = predict[0][0]\n",
    "    if predict > 0.5:\n",
    "        predict = 1\n",
    "    else:\n",
    "        predict = 0\n",
    "        \n",
    "    if predict == expect:\n",
    "        g += 1.0\n",
    "    \n",
    "#     print(f'Expect {expect}, predict {predict}')\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('accuracy:')\n",
    "print(g / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-09T16:00:43.342Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('mlp_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-tensorflow2]",
   "language": "python",
   "name": "conda-env-anaconda3-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
